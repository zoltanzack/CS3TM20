{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CW_Task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoltanzack/CS3TM20/blob/master/CW_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhgYu18FM-_L"
      },
      "source": [
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXdrc29D71MH"
      },
      "source": [
        "# **Steps outline**\n",
        "1. Download your data set by inputting your student number.\n",
        "2. Process your text data, extract features, convert them into vectors\n",
        "3. Modeling, train models on the data set (select model, tune different parameters)\n",
        "4. Process your text data, extract features, convert them into vectors\n",
        "5. Analysis and discussions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934uDa63sRnp"
      },
      "source": [
        "# Step 1: Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxDjTTcDrtVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5aceee-38b2-468a-8235-07c350d8fa35"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train',  categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8VrHduYC-kd"
      },
      "source": [
        "**This is how to identify which data set to use (Please copy  the following information in report front   page).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvU6i2KNHzC4",
        "outputId": "9f8f6244-282a-41cb-c06a-ed8a2a63b86c"
      },
      "source": [
        "index=input('type your student number?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type your student number?12345678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOkb3xh6IqHa",
        "outputId": "5a6dc40d-5df9-4ce6-b037-2279fff499c1"
      },
      "source": [
        "x=divmod(int(index),4)\n",
        "yourdata1=x[1]\n",
        "y=divmod(int(index),3)\n",
        "yourdata2=y[1]\n",
        "print('This is your data set index ----> (', x[1], y[1], ')' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is your data set index ----> ( 2 0 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXPpVRSGAPM7",
        "outputId": "d0edc599-cc43-4cc9-f545-436669b8dcff"
      },
      "source": [
        "data1= twenty_train.target_names[x[1]]\n",
        "data2= twenty_train.target_names[y[1]]\n",
        "categories1=[data1,data2]\n",
        "print(categories1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sci.med', 'alt.atheism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNjBtO7-DOsu"
      },
      "source": [
        "**Your front page data information Ends here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1jAHpjtaSPu"
      },
      "source": [
        "# Step 2 Process your text data, extract features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El_vU9NocxVC"
      },
      "source": [
        "# 2.1 An example of preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDHwDyKzNirS"
      },
      "source": [
        "**An example is provided. You have modify to apply to data set based on your student number.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC3yT07PJnKp",
        "outputId": "ec1a31ad-e576-4f9f-b469-ea6be48dd89b"
      },
      "source": [
        "# write your own NLP precessing examples with  preprocessing techniques.\n",
        "\n",
        "dataset=twenty_train.data[6]\n",
        "print(dataset)\n",
        "# please use own data set by replacing 6 to one of yours.\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# tokenize: search: nltk tokenize \n",
        "example = \"This is an example sentence.\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "example_tokenize= word_tokenize(example) #twenty_train.data[1]\n",
        "print(\"-------------------------tokenize:\")\n",
        "print(example_tokenize)\n",
        "\n",
        "\n",
        "# stemmer: search: nltk stemmer  \n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "example_stem = stemmer.stem(example)\n",
        "print(\"-------------------------stem:\")\n",
        "print(example_stem)\n",
        "\n",
        "# tf-idf: search: scikit learn tf-idf https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
        "# search scikit learn CountVectorizer example: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \n",
        "\n",
        "# pos_taging: search: nltk pos tagging example\n",
        "example_posTag=nltk.pos_tag(example_tokenize)\n",
        "print(\"-------------------------pos_taging:\")\n",
        "print(example_posTag)\n",
        "\n",
        "# consituency parsing, chunking\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(example_posTag)\n",
        "print(result)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: jodfishe@silver.ucs.indiana.edu (joseph dale fisher)\n",
            "Subject: Re: anger\n",
            "Organization: Indiana University\n",
            "Lines: 34\n",
            "\n",
            "In article <Apr.17.01.10.44.1993.2232@geneva.rutgers.edu> news@cbnewsk.att.com writes:\n",
            ">>Paul Conditt writes:\n",
            "[insert deletion of Paul's and Aaron's discourse on anger, ref Galatians\n",
            "5:19-20]\n",
            ">\n",
            ">I don't know why it is so obvious.  We are not speaking of acts of the \n",
            ">flesh.  We are just speaking of emotions.  Emotions are not of themselves\n",
            ">moral or immoral, good or bad.  Emotions just are.  The first step is\n",
            ">not to label his emotion as good or bad or to numb ourselves so that\n",
            ">we hide our true feelings, it is to accept ourselves as we are, as God\n",
            ">accepts us.  \n",
            "\n",
            "Oh, but they definitely can be.  Please look at Colossians 3:5-10 and\n",
            "Ephesians 4:25-27.  Emotions can be controlled and God puts very strong\n",
            "emphasis on self-control, otherwise, why would he have Paul write to\n",
            "Timothy so much about making sure to teach self-control? \n",
            "\n",
            "[insert deletion of remainder of paragraph]\n",
            "\n",
            ">\n",
            ">Re-think it, Aaron.  Don't be quick to judge.  He has forgiven those with\n",
            ">AIDS, he has dealt with and taken responsibility for his feelings and made\n",
            ">appropriate choices for action on such feelings.  He has not given in to\n",
            ">his anger.\n",
            "\n",
            "Please, re-think and re-read for yourself, Joe.  Again, the issue is\n",
            "self-control especially over feelings and actions, for our actions stem\n",
            "from our feelings in many instances.  As for God giving in to his anger,\n",
            "that comes very soon.\n",
            "\n",
            ">\n",
            ">Joe Moore\n",
            "\n",
            "Joe Fisher\n",
            "\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "-------------------------tokenize:\n",
            "['This', 'is', 'an', 'example', 'sentence', '.']\n",
            "-------------------------stem:\n",
            "this is an example sentence.\n",
            "-------------------------pos_taging:\n",
            "[('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('.', '.')]\n",
            "(S This/DT is/VBZ (NP an/DT example/NN) (NP sentence/NN) ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wvWHranFRpG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7VpnVNpKuUt"
      },
      "source": [
        "#2.2 NLP Preprocesssing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hmXyhqwOaUQ"
      },
      "source": [
        "**Some preprocessing are provided for convenience. Please include why NLP preprocessing is in your report. Explain what techniques have been applied in your coursework.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8u5y9adK3tc",
        "outputId": "1db9de0f-9440-4c91-adf4-3568d4c932fd"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "stopwordEn = stopwords.words('english')\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "def lemmaWord(word):\n",
        "    lemma = wordnet.morphy(word)\n",
        "    if lemma is not None:\n",
        "        return lemma\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "def stemWord(word):\n",
        "    stem = stemmer.stem(word)\n",
        "    if stem is not None:\n",
        "        return stem\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "def processText(text,lemma=False, gram=1, rmStop=True): # default remove stop words\n",
        "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', text, flags=re.MULTILINE) #delete URL, #hashtag# , and @xxx\n",
        "    tokens = word_tokenize(text)\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\n",
        "    new_tokens = []\n",
        "    stoplist = stopwordEn if rmStop else []\n",
        "    for i in tokens:\n",
        "      i = i.lower()\n",
        "      if i.isalpha() and (i not in stoplist or i in whitelist):  #i not in ['.',',',';']  and (...)\n",
        "        if lemma: i = lemmaWord(i)\n",
        "        new_tokens.append(i)\n",
        "    del tokens\n",
        "    # tokens = [lemmaWord(i.lower()) if lemma else i.lower() for i in tokens if (i.lower() not in stoplist or i.lower() in whitelist) and i.isalpha()]\n",
        "    if gram<=1:\n",
        "        return new_tokens\n",
        "    else:\n",
        "        return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]\n",
        "\n",
        "       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKFoZaWSVrnq"
      },
      "source": [
        "def getTags(text):\n",
        "  token = word_tokenize(text)\n",
        "  token = [l.lower() for l in token]\n",
        "  train_tags = nltk.pos_tag(token)\n",
        "  return [i[1] for i in train_tags]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8mwYOcFcS02",
        "outputId": "f8bc0267-76fd-4b88-b8c6-7c518e12e0ec"
      },
      "source": [
        "print(processText(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['joseph', 'dale', 'fisher', 'subject', 'anger', 'organization', 'indiana', 'university', 'lines', 'article', 'writes', 'paul', 'conditt', 'writes', 'insert', 'deletion', 'paul', 'aaron', 'discourse', 'anger', 'ref', 'galatians', 'know', 'obvious', 'not', 'speaking', 'acts', 'flesh', 'speaking', 'emotions', 'emotions', 'not', 'moral', 'immoral', 'good', 'bad', 'emotions', 'first', 'step', 'not', 'label', 'emotion', 'good', 'bad', 'numb', 'hide', 'true', 'feelings', 'accept', 'god', 'accepts', 'us', 'oh', 'definitely', 'please', 'look', 'colossians', 'ephesians', 'emotions', 'controlled', 'god', 'puts', 'strong', 'emphasis', 'otherwise', 'would', 'paul', 'write', 'timothy', 'much', 'making', 'sure', 'teach', 'insert', 'deletion', 'remainder', 'paragraph', 'aaron', 'quick', 'judge', 'forgiven', 'aids', 'dealt', 'taken', 'responsibility', 'feelings', 'made', 'appropriate', 'choices', 'action', 'feelings', 'not', 'given', 'anger', 'please', 'joe', 'issue', 'especially', 'feelings', 'actions', 'actions', 'stem', 'feelings', 'many', 'instances', 'god', 'giving', 'anger', 'comes', 'soon', 'joe', 'moore', 'joe', 'fisher']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZRqySa8cbr1",
        "outputId": "175e1cd4-7aed-4cea-8f5f-7ce2524838f1"
      },
      "source": [
        "print(getTags(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['IN', ':', 'NN', 'VBZ', 'NN', '(', 'JJ', 'NN', 'NN', ')', 'NN', ':', 'NN', ':', 'NN', 'NN', ':', 'NN', 'NN', 'NNS', ':', 'CD', 'IN', 'NN', 'NNP', 'NN', 'NNP', 'NN', 'NNP', 'NN', 'NNP', 'NN', 'VBZ', ':', 'NN', 'NN', 'NN', 'NN', 'VBZ', ':', 'NN', 'JJ', 'NN', 'IN', 'NN', 'POS', 'CC', 'NN', 'POS', 'NN', 'IN', 'NN', ',', 'JJ', 'NNS', 'JJ', 'NNP', 'NNP', 'NNP', 'NN', 'VBP', 'RB', 'VB', 'WRB', 'PRP', 'VBZ', 'RB', 'JJ', '.', 'PRP', 'VBP', 'RB', 'VBG', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'NN', '.', 'PRP', 'VBP', 'RB', 'VBG', 'IN', 'NNS', '.', 'NNS', 'VBP', 'RB', 'IN', 'PRP', 'JJ', 'JJ', 'CC', 'JJ', ',', 'JJ', 'CC', 'JJ', '.', 'NNS', 'RB', 'VBP', '.', 'DT', 'JJ', 'NN', 'VBZ', 'RB', 'RB', 'TO', 'VB', 'PRP$', 'NN', 'IN', 'JJ', 'CC', 'JJ', 'CC', 'TO', 'VB', 'PRP', 'RB', 'IN', 'NN', 'PRP', 'VBP', 'PRP$', 'JJ', 'NNS', ',', 'PRP', 'VBZ', 'TO', 'VB', 'PRP', 'IN', 'PRP', 'VBP', ',', 'IN', 'JJ', 'NN', 'NNS', 'PRP', '.', 'UH', ',', 'CC', 'PRP', 'RB', 'MD', 'VB', '.', 'VB', 'NN', 'IN', 'NNS', 'JJ', 'CC', 'VBZ', 'JJ', '.', 'NNS', 'MD', 'VB', 'VBN', 'CC', 'NN', 'VBZ', 'RB', 'JJ', 'NN', 'IN', 'NN', ',', 'RB', ',', 'WRB', 'MD', 'PRP', 'VB', 'VBN', 'NN', 'TO', 'VB', 'RB', 'JJ', 'IN', 'VBG', 'JJ', 'TO', 'VB', 'NN', '.', 'JJ', 'JJ', 'NN', 'IN', 'NN', 'IN', 'NN', 'NNP', 'NNP', 'NNP', 'NN', 'PRP', ',', 'RB', '.', 'VBP', 'RB', 'VB', 'JJ', 'TO', 'VB', '.', 'PRP', 'VBZ', 'VBN', 'DT', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBZ', 'VBN', 'IN', 'CC', 'VBN', 'NN', 'IN', 'PRP$', 'NNS', 'CC', 'VBD', 'NNP', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'JJ', 'NNS', '.', 'PRP', 'VBZ', 'RB', 'VBN', 'IN', 'TO', 'VB', 'PRP$', 'NN', '.', 'NN', ',', 'NN', 'CC', 'NN', 'IN', 'PRP', ',', 'NN', '.', 'RB', ',', 'DT', 'NN', 'VBZ', 'JJ', 'RB', 'IN', 'NNS', 'CC', 'NNS', ',', 'IN', 'PRP$', 'NNS', 'VBP', 'IN', 'PRP$', 'NNS', 'IN', 'JJ', 'NNS', '.', 'IN', 'IN', 'NN', 'VBG', 'IN', 'TO', 'PRP$', 'NN', ',', 'WDT', 'VBZ', 'RB', 'RB', '.', 'JJ', 'JJ', 'NN', 'NN', 'NN', 'NN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44xTvpLa_UC9"
      },
      "source": [
        "# Step 2: Build a Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g5g93owSogu"
      },
      "source": [
        "**Modify the block code below to your choice of classifier [link text](https://https://www.nltk.org/book/ch06.html) **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGs1A8S1TMTi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9HMKvgGMHPB",
        "outputId": "50e53088-c293-4eab-acd6-83cd63bf9f53"
      },
      "source": [
        "print(categories1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sci.med', 'alt.atheism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PDFkEEiL1GQ"
      },
      "source": [
        "#twenty_train1 = fetch_20newsgroups(subset='train',  categories=categories1, shuffle=True, random_state=42)\n",
        "#twenty_test1 = fetch_20newsgroups(subset='test',  categories=categories1, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNm3axlhdzlF"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "\n",
        "# Level: lexicon, model: tf-idf\n",
        "text_clf = Pipeline([\n",
        "    # add your code about text processing           \n",
        "    ('vect', CountVectorizer(analyzer=processText)), \n",
        "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "\n",
        "    # change your classifier here, search: sklearn logistic regression example\n",
        "    ('clf', SGDClassifier())\n",
        "   #  ('clf', LogisticRegression())\n",
        "\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vuq37Bf3Qjpn",
        "outputId": "7698e302-e8d8-4e35-964c-164ab1f6a711"
      },
      "source": [
        "# To train the model \n",
        "text_clf.fit(twenty_train.data, twenty_train.target)\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer=<function processText at 0x7f449db58560>,\n",
              "                                 binary=False, decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b...\n",
              "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
              "                               fit_intercept=True, l1_ratio=0.15,\n",
              "                               learning_rate='optimal', loss='hinge',\n",
              "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "                               penalty='l2', power_t=0.5, random_state=None,\n",
              "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
              "                               verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjQ8DmPNRUuJ"
      },
      "source": [
        "# Step 3: Make Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMdoIHjMRWce"
      },
      "source": [
        "# To make prediction with dev/test set\n",
        "predicted = text_clf.predict(twenty_test.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GXHJHqoBmyJ"
      },
      "source": [
        "# Step 4: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "LdB9js0QDErf",
        "outputId": "6205a904-0e14-4684-dcb9-cb6c7635eecf"
      },
      "source": [
        "# To evaluate your prediction on dev set\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy:\", metrics.accuracy_score(twenty_test.target, predicted))\n",
        "\n",
        "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n",
        "\n",
        "# confusion class\n",
        "pd.DataFrame(metrics.confusion_matrix(twenty_test.target, predicted),\n",
        "             columns=twenty_test.target_names,index=twenty_test.target_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9134487350199734\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "           alt.atheism       0.90      0.82      0.86       319\n",
            "         comp.graphics       0.93      0.96      0.94       389\n",
            "               sci.med       0.95      0.92      0.93       396\n",
            "soc.religion.christian       0.88      0.94      0.91       398\n",
            "\n",
            "              accuracy                           0.91      1502\n",
            "             macro avg       0.91      0.91      0.91      1502\n",
            "          weighted avg       0.91      0.91      0.91      1502\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alt.atheism</th>\n",
              "      <th>comp.graphics</th>\n",
              "      <th>sci.med</th>\n",
              "      <th>soc.religion.christian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>alt.atheism</th>\n",
              "      <td>261</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comp.graphics</th>\n",
              "      <td>8</td>\n",
              "      <td>373</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sci.med</th>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>363</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soc.religion.christian</th>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        alt.atheism  ...  soc.religion.christian\n",
              "alt.atheism                     261  ...                      44\n",
              "comp.graphics                     8  ...                       2\n",
              "sci.med                           9  ...                       7\n",
              "soc.religion.christian           11  ...                     375\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCLCqFXPQsRq"
      },
      "source": [
        "# Step 5: Error Analysis and Discussion\n",
        "write down your own obseration about the predictions. Consider both confusion matrix and selected examples. Which classes are predicted correctly or incorrecly, possible explaination, possible solutions \n",
        "\n",
        "Exmaple: 1) Lab Practical, which feature is helpful for female name classification. https://www.nltk.org/book/ch06.html \n",
        "2) research paper: https://github.com/yoonkim/CNN_sentence \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "kvBw9qkKDS-m",
        "outputId": "b0794747-9d21-4562-f049-40e057b764a9"
      },
      "source": [
        "df_pred = pd.DataFrame({'news':twenty_test.data,'prediction':predicted, 'true':twenty_test.target})\n",
        "df_pred[df_pred['true'] != df_pred['prediction']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news</th>\n",
              "      <th>prediction</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>From: \"Gabriel D. Underwood\" &lt;gabe+@CMU.EDU&gt;\\n...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>From: swf@elsegundoca.ncr.com (Stan Friesen)\\n...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: D...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>From: UC512052@mizzou1.missouri.edu (David K. ...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Organization: Penn State University\\nFrom: &lt;RF...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1450</th>\n",
              "      <td>From: alan.barclay@almac.co.uk (Alan Barclay)\\...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>From: pww@spacsun.rice.edu (Peter Walker)\\nSub...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1480</th>\n",
              "      <td>From: wilsonr@logica.co.uk\\nSubject: Re: What ...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1492</th>\n",
              "      <td>From: kmr4@po.CWRU.edu (Keith M. Ryan)\\nSubjec...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>130 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   news  prediction  true\n",
              "12    From: \"Gabriel D. Underwood\" <gabe+@CMU.EDU>\\n...           3     2\n",
              "15    From: swf@elsegundoca.ncr.com (Stan Friesen)\\n...           2     0\n",
              "19    From: mathew <mathew@mantis.co.uk>\\nSubject: D...           3     0\n",
              "26    From: UC512052@mizzou1.missouri.edu (David K. ...           2     1\n",
              "36    Organization: Penn State University\\nFrom: <RF...           0     2\n",
              "...                                                 ...         ...   ...\n",
              "1438  From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...           0     3\n",
              "1450  From: alan.barclay@almac.co.uk (Alan Barclay)\\...           1     2\n",
              "1455  From: pww@spacsun.rice.edu (Peter Walker)\\nSub...           3     0\n",
              "1480  From: wilsonr@logica.co.uk\\nSubject: Re: What ...           3     0\n",
              "1492  From: kmr4@po.CWRU.edu (Keith M. Ryan)\\nSubjec...           3     0\n",
              "\n",
              "[130 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRIRg2rufjPu"
      },
      "source": [
        "#References:  \n",
        "\n",
        "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html \n",
        "\n",
        "https://www.nltk.org/book/ch06.html \n",
        "\n",
        "search: Other online resources: \n",
        "\n",
        "https://towardsdatascience.com/setting-up-text-preprocessing-pipeline-using-scikit-learn-and-spacy-e09b9b76758f \n",
        " \n",
        "sentiment analysis scikit learn \n",
        "\n",
        "scikit learn or nltk + NLP techniques \n",
        "\n",
        "python + NLP techniques\n",
        "\n",
        "scikit learn logistic regression\n",
        "\n",
        "\n"
      ]
    }
  ]
}